services:
  # --------------------------------------------------------------------------
  # Application Service: The resource-server we built.
  # --------------------------------------------------------------------------
  resource-server:
    build:
      context: .
      dockerfile: resource-server/Dockerfile
    container_name: resource-server
    env_file:
      - .env
    ports:
      - "8081:8081" # Business API port
    environment:
      # We enable the 'chaos' profile to activate the ChaosController for demonstration.
      - SPRING_PROFILES_ACTIVE=chaos
      # OpenTelemetry Agent Configuration
      - JAVA_TOOL_OPTIONS=-javaagent:/app/opentelemetry-javaagent.jar
      - OTEL_SERVICE_NAME=resource-server
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://alloy:4317
      - OTEL_EXPORTER_OTLP_PROTOCOL=grpc
      - OTEL_METRICS_EXPORTER=none
      - OTEL_LOGS_EXPORTER=none
    volumes:
      - ./config/otel/opentelemetry-javaagent.jar:/app/opentelemetry-javaagent.jar:ro
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9092/actuator/health" ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
    depends_on:
      keycloak:
        condition: service_healthy
      alloy:
        condition: service_started
    networks:
      - lab-net

  # --------------------------------------------------------------------------
  # Web Client Service: The OIDC client application
  # --------------------------------------------------------------------------
  web-client:
    build:
      context: .
      dockerfile: web-client/Dockerfile
    container_name: web-client
    env_file:
      - .env
    ports:
      - "8082:8082"
    environment:
      # OpenTelemetry Agent Configuration
      - JAVA_TOOL_OPTIONS=-javaagent:/app/opentelemetry-javaagent.jar
      - OTEL_SERVICE_NAME=web-client
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://alloy:4317
      - OTEL_EXPORTER_OTLP_PROTOCOL=grpc
      - OTEL_METRICS_EXPORTER=none
      - OTEL_LOGS_EXPORTER=none
    volumes:
      # Mount the agent JAR into a known location in the container.
      - ./config/otel/opentelemetry-javaagent.jar:/app/opentelemetry-javaagent.jar:ro
    depends_on:
      keycloak:
        condition: service_healthy
      resource-server:
        condition: service_healthy
      alloy:
        condition: service_started
    networks:
      - lab-net

  # --------------------------------------------------------------------------
  # Identity & Access Management Service: Keycloak
  # --------------------------------------------------------------------------
  keycloak:
    image: quay.io/keycloak/keycloak:26.3.3
    container_name: keycloak
    env_file:
      - .env
    environment:
      - KC_HEALTH_ENABLED=true
      # Database Configuration
      - KC_DB=postgres
      - KC_DB_URL_HOST=postgres
      - KC_DB_URL_DATABASE=${POSTGRES_DB}
      - KC_DB_USERNAME=${POSTGRES_USER}
      - KC_DB_PASSWORD=${POSTGRES_PASSWORD}
      - KC_DB_SCHEMA=public
    volumes:
      - ./config/keycloak:/opt/keycloak/data/import # Mount the realm export file for automatic import on startup.
    command:
      - start-dev # Disables enforced HTTPS
      - --import-realm # Enables the realm import to avoid fulfilling it manually
    healthcheck:
      # A bit tricky, but necessary as image does not contain curl or wget
      test: [ "CMD-SHELL", "exec 3<>/dev/tcp/127.0.0.1/9000 && echo -e 'GET /health/ready HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n' >&3 && grep 'HTTP/1.1 200 OK' <&3" ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.keycloak.rule=Host(`keycloak.local`)"
      - "traefik.http.routers.keycloak.entrypoints=web"
      - "traefik.http.services.keycloak.loadbalancer.server.port=8080"
    depends_on:
      - postgres
    networks:
      - lab-net

  # --------------------------------------------------------------------------
  # Reverse Proxy Traefik for Keycloak
  # --------------------------------------------------------------------------
  traefik:
    image: traefik:v3.5
    container_name: traefik-proxy
    command:
      - "--api.insecure=true"
      - "--providers.docker=true"
      - "--entrypoints.web.address=:80"
    ports:
      - "80:80" # Public web traffic for Keycloak
      - "8080:8080" # Dashboard
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
    networks:
      lab-net:
        aliases:
          - keycloak.local

  # --------------------------------------------------------------------------
  # Database Service for Keycloak
  # --------------------------------------------------------------------------
  postgres:
    image: postgres:17.6-alpine
    container_name: postgres
    env_file:
      - .env
    volumes:
      - keycloak-db-data:/var/lib/postgresql/data
    networks:
      - lab-net

  # --------------------------------------------------------------------------
  # Prometheus Service: For metrics collection.
  # --------------------------------------------------------------------------
  prometheus:
    image: prom/prometheus:v3.5.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    secrets:
      - actuator_username
      - actuator_password
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--web.enable-remote-write-receiver'
    networks:
      - lab-net

  # --------------------------------------------------------------------------
  # Alertmanager Service: For alert management and routing.
  # --------------------------------------------------------------------------
  alertmanager:
    image: prom/alertmanager:v0.28.1
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./config/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:9093/-/ready || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - lab-net

  # --------------------------------------------------------------------------
  # Grafana Tempo Service: For distributed tracing.
  # --------------------------------------------------------------------------
  tempo:
    image: grafana/tempo:2.8.2
    container_name: tempo
    ports:
      - "3200:3200"   # Tempo HTTP
      - "4317:4317"   # OTLP gRPC
      - "9411:9411"   # Zipkin
    volumes:
      - tempo-data:/var/tempo
      - ./config/tempo/tempo.yml:/etc/tempo.yml
    command: [ "-config.file=/etc/tempo.yml" ] # Config file will define storage
    networks:
      - lab-net

  # --------------------------------------------------------------------------
  # Grafana Loki Service: For centralized log aggregation.
  # --------------------------------------------------------------------------
  loki:
    image: grafana/loki:3.5.3
    container_name: loki
    # The auth flag is the way to run Loki in single-tenant mode
    command: "-config.file=/etc/loki/loki-config.yml -auth.enabled=false"
    volumes:
      - ./config/loki/loki-config.yml:/etc/loki/loki-config.yml:ro
      - loki-data:/loki/data
    networks:
      - lab-net

  # --------------------------------------------------------------------------
  # Grafana Alloy Service: The unified observability agent.
  # --------------------------------------------------------------------------
  alloy:
    image: grafana/alloy:v1.10.2
    container_name: alloy
    volumes:
      # =============================== SECURITY DISCLAIMER ===============================
      # The read-only mount of the Docker socket is a privileged operation.
      # For this lab, we accept this risk to enable a portable & simplified experience with automatic
      # service discovery. For production environments, a non-socket-based approach like
      # using a native Docker logging driver, or sidecar would be a more secure alternative.
      # ===================================================================================
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./config/alloy/alloy-config.river:/etc/alloy/config.river:ro
    command: "run /etc/alloy/config.river"
    depends_on:
      - loki
      - tempo
    networks:
      - lab-net


  # --------------------------------------------------------------------------
  # Grafana Service: For metrics visualization.
  # --------------------------------------------------------------------------
  grafana:
    image: grafana/grafana-oss:12.1.1
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana/provisioning/datasources/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
      - ./config/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      prometheus:
        condition: service_started
      tempo:
        condition: service_started
      loki:
        condition: service_started
      alertmanager:
        condition: service_healthy
    networks:
      - lab-net

secrets:
  actuator_username:
    environment: "ACTUATOR_USERNAME"
  actuator_password:
    environment: "ACTUATOR_PASSWORD"

volumes:
  grafana-data:
  keycloak-db-data:
  tempo-data:
  loki-data:
  alertmanager-data:

networks:
  lab-net:
    driver: bridge